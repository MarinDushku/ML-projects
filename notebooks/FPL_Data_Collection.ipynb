{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# üèÜ FPL-AI Data Collection Notebook\n",
    "\n",
    "## Overview\n",
    "This notebook collects comprehensive Fantasy Premier League data from multiple free sources:\n",
    "- **Official FPL API**: Player stats, fixtures, gameweek data\n",
    "- **Injury Data**: Player availability and fitness status\n",
    "- **Advanced Stats**: Team strength and performance metrics\n",
    "\n",
    "## Expected Runtime: 10-15 minutes\n",
    "## Data Sources: 100% Free APIs and Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell_1_setup"
   },
   "outputs": [],
   "source": [
    "# Cell 1: Environment Setup and Google Drive Integration\n",
    "print(\"üöÄ Setting up FPL-AI Data Collection Environment...\")\n",
    "\n",
    "# Install required packages\n",
    "!pip install -q requests beautifulsoup4 pandas numpy tqdm lxml fake-useragent\n",
    "!pip install -q pyyaml joblib\n",
    "\n",
    "# Mount Google Drive for data persistence\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create project directory in Drive\n",
    "import os\n",
    "project_dir = '/content/drive/MyDrive/FPL_AI_Project'\n",
    "os.makedirs(project_dir, exist_ok=True)\n",
    "os.makedirs(f'{project_dir}/data/raw', exist_ok=True)\n",
    "os.makedirs(f'{project_dir}/data/processed', exist_ok=True)\n",
    "os.makedirs(f'{project_dir}/models', exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Environment setup complete!\")\n",
    "print(f\"üìÅ Project directory: {project_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell_2_imports"
   },
   "outputs": [],
   "source": [
    "# Cell 2: Import Libraries and Configure APIs\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# API Configuration\n",
    "FPL_API_BASE = \"https://fantasy.premierleague.com/api/\"\n",
    "PREMIER_INJURIES_URL = \"https://www.premierinjuries.com/injury-table.php\"\n",
    "RATE_LIMIT_DELAY = 1.0  # seconds between requests\n",
    "\n",
    "# Headers for web scraping\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "print(\"üìö Libraries imported successfully!\")\n",
    "print(f\"üåê FPL API Base: {FPL_API_BASE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell_3_fpl_api"
   },
   "outputs": [],
   "source": [
    "# Cell 3: FPL API Data Collection Functions\n",
    "\n",
    "class FPLDataCollector:\n",
    "    \"\"\"Comprehensive FPL data collector with rate limiting.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_url=FPL_API_BASE):\n",
    "        self.base_url = base_url\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update(HEADERS)\n",
    "        \n",
    "    def _make_request(self, endpoint):\n",
    "        \"\"\"Make rate-limited request to FPL API.\"\"\"\n",
    "        time.sleep(RATE_LIMIT_DELAY)\n",
    "        \n",
    "        try:\n",
    "            response = self.session.get(f\"{self.base_url}{endpoint}\", timeout=30)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error fetching {endpoint}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def get_bootstrap_data(self):\n",
    "        \"\"\"Get comprehensive bootstrap data.\"\"\"\n",
    "        print(\"üìä Fetching bootstrap static data...\")\n",
    "        return self._make_request(\"bootstrap-static/\")\n",
    "    \n",
    "    def get_fixtures(self):\n",
    "        \"\"\"Get fixture data.\"\"\"\n",
    "        print(\"‚öΩ Fetching fixtures data...\")\n",
    "        return self._make_request(\"fixtures/\")\n",
    "    \n",
    "    def get_gameweek_live(self, gameweek):\n",
    "        \"\"\"Get live gameweek data.\"\"\"\n",
    "        print(f\"üî¥ Fetching live data for gameweek {gameweek}...\")\n",
    "        return self._make_request(f\"event/{gameweek}/live/\")\n",
    "    \n",
    "    def collect_historical_gameweeks(self, start_gw=1, end_gw=10):\n",
    "        \"\"\"Collect historical gameweek data.\"\"\"\n",
    "        print(f\"üìà Collecting historical data from GW{start_gw} to GW{end_gw}...\")\n",
    "        \n",
    "        all_data = []\n",
    "        for gw in tqdm(range(start_gw, end_gw + 1), desc=\"Gameweeks\"):\n",
    "            gw_data = self.get_gameweek_live(gw)\n",
    "            if gw_data and 'elements' in gw_data:\n",
    "                for element in gw_data['elements']:\n",
    "                    stats = element['stats']\n",
    "                    stats['player_id'] = element['id']\n",
    "                    stats['gameweek'] = gw\n",
    "                    all_data.append(stats)\n",
    "        \n",
    "        return pd.DataFrame(all_data) if all_data else pd.DataFrame()\n",
    "\n",
    "# Initialize collector\n",
    "collector = FPLDataCollector()\n",
    "print(\"‚úÖ FPL Data Collector initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell_4_injury_scraper"
   },
   "outputs": [],
   "source": [
    "# Cell 4: Injury Data Scraping Functions\n",
    "\n",
    "class InjuryDataScraper:\n",
    "    \"\"\"Scraper for Premier League injury data.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update(HEADERS)\n",
    "        \n",
    "        # Team name standardization\n",
    "        self.team_mapping = {\n",
    "            'Man City': 'Manchester City',\n",
    "            'Man Utd': 'Manchester United',\n",
    "            'Newcastle': 'Newcastle United',\n",
    "            'Brighton': 'Brighton & Hove Albion',\n",
    "            'Nott\\'m Forest': 'Nottingham Forest',\n",
    "            'Sheffield Utd': 'Sheffield United',\n",
    "            'West Ham': 'West Ham United',\n",
    "            'Wolves': 'Wolverhampton Wanderers'\n",
    "        }\n",
    "        \n",
    "        # Injury severity mapping\n",
    "        self.severity_mapping = {\n",
    "            'knock': 1, 'minor': 1, 'fatigue': 1, 'slight': 1,\n",
    "            'muscle': 2, 'strain': 2, 'sprain': 2, 'bruise': 2,\n",
    "            'fracture': 4, 'break': 4, 'torn': 4,\n",
    "            'rupture': 5, 'surgery': 5, 'operation': 5, 'long-term': 5\n",
    "        }\n",
    "    \n",
    "    def scrape_premier_injuries(self):\n",
    "        \"\"\"Scrape injury data from Premier Injuries website.\"\"\"\n",
    "        print(\"üè• Scraping injury data from Premier Injuries...\")\n",
    "        \n",
    "        try:\n",
    "            time.sleep(RATE_LIMIT_DELAY)\n",
    "            response = self.session.get(PREMIER_INJURIES_URL, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Find injury table (try multiple selectors)\n",
    "            table = (soup.find('table', class_='injury-table') or \n",
    "                    soup.find('table', id='injury-table') or\n",
    "                    soup.find('table'))\n",
    "            \n",
    "            if not table:\n",
    "                print(\"‚ö†Ô∏è Could not find injury table\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            injuries = []\n",
    "            rows = table.find_all('tr')[1:]  # Skip header\n",
    "            \n",
    "            for row in rows:\n",
    "                cells = row.find_all(['td', 'th'])\n",
    "                if len(cells) >= 4:\n",
    "                    injury_data = {\n",
    "                        'player_name': cells[0].get_text(strip=True),\n",
    "                        'team': cells[1].get_text(strip=True),\n",
    "                        'injury_type': cells[2].get_text(strip=True),\n",
    "                        'status': cells[3].get_text(strip=True),\n",
    "                        'expected_return': cells[4].get_text(strip=True) if len(cells) > 4 else '',\n",
    "                        'last_updated': datetime.now().strftime('%Y-%m-%d')\n",
    "                    }\n",
    "                    injuries.append(injury_data)\n",
    "            \n",
    "            df = pd.DataFrame(injuries)\n",
    "            if not df.empty:\n",
    "                df = self._process_injury_data(df)\n",
    "            \n",
    "            print(f\"‚úÖ Scraped {len(df)} injury records\")\n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error scraping injury data: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def _process_injury_data(self, df):\n",
    "        \"\"\"Process and enhance injury data.\"\"\"\n",
    "        # Standardize team names\n",
    "        df['team_standardized'] = df['team'].map(self.team_mapping).fillna(df['team'])\n",
    "        \n",
    "        # Calculate injury severity\n",
    "        df['injury_severity'] = df['injury_type'].str.lower().apply(\n",
    "            lambda x: max([self.severity_mapping.get(keyword, 2) \n",
    "                          for keyword in self.severity_mapping.keys() \n",
    "                          if keyword in str(x)], default=2)\n",
    "        )\n",
    "        \n",
    "        # Estimate availability probability\n",
    "        df['availability_probability'] = 1.0 - (df['injury_severity'] * 0.15)\n",
    "        df['availability_probability'] = df['availability_probability'].clip(0, 1)\n",
    "        \n",
    "        return df\n",
    "\n",
    "# Initialize injury scraper\n",
    "injury_scraper = InjuryDataScraper()\n",
    "print(\"‚úÖ Injury Data Scraper initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell_5_data_collection"
   },
   "outputs": [],
   "source": [
    "# Cell 5: Execute Comprehensive Data Collection\n",
    "\n",
    "print(\"üéØ Starting comprehensive FPL data collection...\")\n",
    "print(\"‚è±Ô∏è Estimated time: 5-10 minutes\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Dictionary to store all collected data\n",
    "collected_data = {}\n",
    "\n",
    "# 1. Get bootstrap static data (players, teams, gameweeks)\n",
    "print(\"\\nüìä Step 1: Collecting bootstrap data...\")\n",
    "bootstrap_data = collector.get_bootstrap_data()\n",
    "\n",
    "if bootstrap_data:\n",
    "    # Extract individual datasets\n",
    "    players_df = pd.DataFrame(bootstrap_data['elements'])\n",
    "    teams_df = pd.DataFrame(bootstrap_data['teams'])\n",
    "    gameweeks_df = pd.DataFrame(bootstrap_data['events'])\n",
    "    positions_df = pd.DataFrame(bootstrap_data['element_types'])\n",
    "    \n",
    "    # Enhance players data with team and position info\n",
    "    players_df = players_df.merge(\n",
    "        teams_df[['id', 'name', 'short_name']].rename(columns={\n",
    "            'id': 'team', 'name': 'team_name', 'short_name': 'team_short'\n",
    "        }), on='team'\n",
    "    )\n",
    "    \n",
    "    players_df = players_df.merge(\n",
    "        positions_df[['id', 'singular_name']].rename(columns={\n",
    "            'id': 'element_type', 'singular_name': 'position'\n",
    "        }), on='element_type'\n",
    "    )\n",
    "    \n",
    "    collected_data['players'] = players_df\n",
    "    collected_data['teams'] = teams_df\n",
    "    collected_data['gameweeks'] = gameweeks_df\n",
    "    \n",
    "    print(f\"‚úÖ Players: {len(players_df)}, Teams: {len(teams_df)}, Gameweeks: {len(gameweeks_df)}\")\nelse:\n",
    "    print(\"‚ùå Failed to collect bootstrap data\")\n",
    "\n",
    "# 2. Get fixtures data\n",
    "print(\"\\n‚öΩ Step 2: Collecting fixtures data...\")\n",
    "fixtures_data = collector.get_fixtures()\n",
    "\n",
    "if fixtures_data:\n",
    "    fixtures_df = pd.DataFrame(fixtures_data)\n",
    "    collected_data['fixtures'] = fixtures_df\n",
    "    print(f\"‚úÖ Fixtures: {len(fixtures_df)}\")\nelse:\n",
    "    print(\"‚ùå Failed to collect fixtures data\")\n",
    "\n",
    "# 3. Get historical gameweek data\n",
    "print(\"\\nüìà Step 3: Collecting historical gameweek data...\")\n",
    "# Determine current gameweek\n",
    "if 'gameweeks' in collected_data:\n",
    "    current_gw = collected_data['gameweeks'][collected_data['gameweeks']['is_current'] == True]\n",
    "    if not current_gw.empty:\n",
    "        current_gameweek = current_gw['id'].iloc[0]\n",
    "        end_gw = min(current_gameweek - 1, 15)  # Don't go beyond GW15 for demo\n",
    "        \n",
    "        historical_df = collector.collect_historical_gameweeks(1, end_gw)\n",
    "        if not historical_df.empty:\n",
    "            collected_data['historical_gameweeks'] = historical_df\n",
    "            print(f\"‚úÖ Historical data: {len(historical_df)} player-gameweek records\")\n",
    "        else:\n",
    "            print(\"‚ùå No historical gameweek data collected\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Could not determine current gameweek\")\n",
    "\n",
    "# 4. Collect injury data\n",
    "print(\"\\nüè• Step 4: Collecting injury data...\")\n",
    "injury_df = injury_scraper.scrape_premier_injuries()\n",
    "\n",
    "if not injury_df.empty:\n",
    "    collected_data['injuries'] = injury_df\n",
    "    print(f\"‚úÖ Injury data: {len(injury_df)} records\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No injury data collected (this is optional)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"üéâ Data collection completed!\")\n",
    "print(f\"üìä Total datasets collected: {len(collected_data)}\")\n",
    "\n",
    "# Display summary\n",
    "for dataset_name, dataset in collected_data.items():\n",
    "    print(f\"  üìÅ {dataset_name}: {len(dataset)} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell_6_save_data"
   },
   "outputs": [],
   "source": [
    "# Cell 6: Save Data to Google Drive and Validation\n",
    "\n",
    "print(\"üíæ Saving collected data to Google Drive...\")\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Save each dataset\n",
    "saved_files = []\n",
    "for dataset_name, dataset in collected_data.items():\n",
    "    if isinstance(dataset, pd.DataFrame) and not dataset.empty:\n",
    "        filename = f\"{project_dir}/data/raw/{timestamp}_{dataset_name}.csv\"\n",
    "        dataset.to_csv(filename, index=False)\n",
    "        saved_files.append(filename)\n",
    "        print(f\"‚úÖ Saved {dataset_name}: {filename}\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'collection_timestamp': timestamp,\n",
    "    'datasets_collected': list(collected_data.keys()),\n",
    "    'total_files': len(saved_files),\n",
    "    'file_paths': saved_files\n",
    "}\n",
    "\n",
    "import json\n",
    "metadata_file = f\"{project_dir}/data/raw/{timestamp}_metadata.json\"\n",
    "with open(metadata_file, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"\\nüìã Metadata saved: {metadata_file}\")\n",
    "\n",
    "# Data validation and summary\n",
    "print(\"\\nüîç Data Validation Summary:\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "if 'players' in collected_data:\n",
    "    players = collected_data['players']\n",
    "    print(f\"üë• Players by position:\")\n",
    "    print(players['position'].value_counts())\n",
    "    print(f\"\\nüí∞ Price range: ¬£{players['now_cost'].min()/10:.1f}m - ¬£{players['now_cost'].max()/10:.1f}m\")\n",
    "\n",
    "if 'historical_gameweeks' in collected_data:\n",
    "    historical = collected_data['historical_gameweeks']\n",
    "    print(f\"\\nüìä Historical data:\")\n",
    "    print(f\"  Gameweeks: {historical['gameweek'].min()} - {historical['gameweek'].max()}\")\n",
    "    print(f\"  Total points range: {historical['total_points'].min()} - {historical['total_points'].max()}\")\n",
    "\n",
    "if 'injuries' in collected_data:\n",
    "    injuries = collected_data['injuries']\n",
    "    print(f\"\\nüè• Injury summary:\")\n",
    "    print(f\"  Teams with injuries: {injuries['team_standardized'].nunique()}\")\n",
    "    print(f\"  Avg availability probability: {injuries['availability_probability'].mean():.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üéØ DATA COLLECTION COMPLETE!\")\n",
    "print(\"üöÄ Ready for feature engineering and model training!\")\n",
    "print(f\"üìÅ All data saved in: {project_dir}/data/raw/\")\n",
    "\n",
    "# Create a downloadable summary\n",
    "summary_data = {\n",
    "    'timestamp': timestamp,\n",
    "    'total_players': len(collected_data.get('players', [])),\n",
    "    'total_fixtures': len(collected_data.get('fixtures', [])),\n",
    "    'historical_records': len(collected_data.get('historical_gameweeks', [])),\n",
    "    'injury_records': len(collected_data.get('injuries', [])),\n",
    "    'next_steps': [\n",
    "        \"Run FPL_Feature_Engineering.ipynb\",\n",
    "        \"Train position-specific models\",\n",
    "        \"Create ensemble predictor\",\n",
    "        \"Build dashboard\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"\\nüìã Collection Summary:\")\n",
    "for key, value in summary_data.items():\n",
    "    if key != 'next_steps':\n",
    "        print(f\"  {key}: {value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 }
}