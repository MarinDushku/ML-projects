{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# ü§ñ FPL-AI Model Training Notebook\n",
    "\n",
    "## Overview\n",
    "This notebook trains position-specific machine learning models for Fantasy Premier League predictions:\n",
    "- **Goalkeeper Model**: Clean sheets, saves, bonus points\n",
    "- **Defender Model**: Clean sheets + attacking returns\n",
    "- **Midfielder Model**: Goals (5pts), assists, creativity\n",
    "- **Forward Model**: Goals (4pts), assists, penalty likelihood\n",
    "\n",
    "## ML Techniques Used:\n",
    "- XGBoost, LightGBM, CatBoost ensemble\n",
    "- Optuna hyperparameter optimization\n",
    "- Time series cross-validation\n",
    "- Custom FPL scoring loss functions\n",
    "- SHAP explainability\n",
    "\n",
    "## Expected Runtime: 30-45 minutes with GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell_1_setup"
   },
   "outputs": [],
   "source": [
    "# Cell 1: Environment Setup and GPU Configuration\n",
    "print(\"üöÄ Setting up FPL-AI Model Training Environment...\")\n",
    "\n",
    "# Check GPU availability\n",
    "import tensorflow as tf\n",
    "print(f\"üî• GPU Available: {tf.config.list_physical_devices('GPU')}\")\n",
    "\n",
    "# Install ML packages\n",
    "!pip install -q xgboost lightgbm catboost optuna\n",
    "!pip install -q scikit-learn==1.3.0 shap lime\n",
    "!pip install -q plotly seaborn matplotlib\n",
    "!pip install -q tqdm ipywidgets\n",
    "\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set project directory\n",
    "import os\n",
    "project_dir = '/content/drive/MyDrive/FPL_AI_Project'\n",
    "os.chdir(project_dir)\n",
    "\n",
    "# Create model directories\n",
    "os.makedirs('models/trained_models/goalkeeper', exist_ok=True)\n",
    "os.makedirs('models/trained_models/defender', exist_ok=True)\n",
    "os.makedirs('models/trained_models/midfielder', exist_ok=True)\n",
    "os.makedirs('models/trained_models/forward', exist_ok=True)\n",
    "os.makedirs('models/ensemble', exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Environment setup complete!\")\n",
    "print(f\"üìÅ Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell_2_imports"
   },
   "outputs": [],
   "source": [
    "# Cell 2: Import Libraries and Load Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "import optuna\n",
    "import shap\n",
    "\n",
    "# Utilities\n",
    "import joblib\n",
    "import json\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üìö All libraries imported successfully!\")\n",
    "\n",
    "# Load the most recent data\n",
    "print(\"\\nüìÇ Loading processed feature data...\")\n",
    "# List available feature files\n",
    "import glob\n",
    "feature_files = glob.glob('data/processed/*features*.csv')\n",
    "if feature_files:\n",
    "    latest_features = max(feature_files, key=os.path.getctime)\n",
    "    features_df = pd.read_csv(latest_features)\n",
    "    print(f\"‚úÖ Loaded features: {latest_features}\")\n",
    "    print(f\"üìä Features shape: {features_df.shape}\")\nelse:\n",
    "    print(\"‚ùå No feature files found. Please run FPL_Feature_Engineering.ipynb first.\")\n",
    "    features_df = pd.DataFrame()\n",
    "\n",
    "# Load historical gameweek data for targets\n",
    "target_files = glob.glob('data/raw/*historical_gameweeks*.csv')\n",
    "if target_files:\n",
    "    latest_targets = max(target_files, key=os.path.getctime)\n",
    "    targets_df = pd.read_csv(latest_targets)\n",
    "    print(f\"‚úÖ Loaded targets: {latest_targets}\")\n",
    "    print(f\"üìä Targets shape: {targets_df.shape}\")\nelse:\n",
    "    print(\"‚ùå No target files found. Please run FPL_Data_Collection.ipynb first.\")\n",
    "    targets_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell_3_base_model"
   },
   "outputs": [],
   "source": [
    "# Cell 3: Base Model Class and Utilities\n",
    "\n",
    "class FPLPositionModel:\n",
    "    \"\"\"Base class for position-specific FPL models.\"\"\"\n",
    "    \n",
    "    def __init__(self, position, target_columns, feature_columns):\n",
    "        self.position = position\n",
    "        self.target_columns = target_columns\n",
    "        self.feature_columns = feature_columns\n",
    "        self.models = {}\n",
    "        self.scalers = {}\n",
    "        self.feature_importance = {}\n",
    "        self.training_metrics = {}\n",
    "        self.is_trained = False\n",
    "        \n",
    "    def prepare_data(self, features_df, targets_df):\n",
    "        \"\"\"Prepare training data for this position.\"\"\"\n",
    "        # Filter by position\n",
    "        position_mask = features_df['position'] == self.position\n",
    "        position_features = features_df[position_mask].copy()\n",
    "        \n",
    "        if position_features.empty:\n",
    "            return None, None\n",
    "        \n",
    "        # Select available features\n",
    "        available_features = [col for col in self.feature_columns if col in position_features.columns]\n",
    "        X = position_features[available_features].fillna(0)\n",
    "        \n",
    "        # Merge with targets\n",
    "        merged_data = position_features[['player_id', 'gameweek']].merge(\n",
    "            targets_df[['player_id', 'gameweek'] + self.target_columns],\n",
    "            on=['player_id', 'gameweek'],\n",
    "            how='inner'\n",
    "        )\n",
    "        \n",
    "        if merged_data.empty:\n",
    "            return None, None\n",
    "        \n",
    "        # Align features with merged data\n",
    "        X = X.loc[X.index.isin(merged_data.index)]\n",
    "        y = merged_data[self.target_columns]\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def optimize_hyperparameters(self, X, y, target, n_trials=50):\n",
    "        \"\"\"Optimize hyperparameters using Optuna.\"\"\"\n",
    "        \n",
    "        def objective(trial):\n",
    "            # Suggest hyperparameters\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "                'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0)\n",
    "            }\n",
    "            \n",
    "            # Create model\n",
    "            model = xgb.XGBRegressor(**params, random_state=42, n_jobs=-1)\n",
    "            \n",
    "            # Time series cross-validation\n",
    "            tscv = TimeSeriesSplit(n_splits=3)\n",
    "            scores = []\n",
    "            \n",
    "            for train_idx, val_idx in tscv.split(X):\n",
    "                X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "                y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "                \n",
    "                model.fit(X_train, y_train)\n",
    "                pred = model.predict(X_val)\n",
    "                score = mean_squared_error(y_val, pred, squared=False)\n",
    "                scores.append(score)\n",
    "            \n",
    "            return np.mean(scores)\n",
    "        \n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "        \n",
    "        return study.best_params\n",
    "    \n",
    "    def train_model(self, X, y, target, optimize=True):\n",
    "        \"\"\"Train model for a specific target.\"\"\"\n",
    "        # Remove missing targets\n",
    "        valid_mask = ~y[target].isna()\n",
    "        X_clean = X[valid_mask]\n",
    "        y_clean = y[target][valid_mask]\n",
    "        \n",
    "        if len(y_clean) < 50:\n",
    "            print(f\"‚ö†Ô∏è Insufficient data for {target}: {len(y_clean)} samples\")\n",
    "            return None\n",
    "        \n",
    "        # Scale features\n",
    "        scaler = RobustScaler()\n",
    "        X_scaled = pd.DataFrame(\n",
    "            scaler.fit_transform(X_clean),\n",
    "            columns=X_clean.columns,\n",
    "            index=X_clean.index\n",
    "        )\n",
    "        self.scalers[target] = scaler\n",
    "        \n",
    "        # Optimize hyperparameters\n",
    "        if optimize:\n",
    "            print(f\"üîß Optimizing hyperparameters for {target}...\")\n",
    "            best_params = self.optimize_hyperparameters(X_scaled, y_clean, target)\n",
    "        else:\n",
    "            best_params = {'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 6}\n",
    "        \n",
    "        # Train final model\n",
    "        model = xgb.XGBRegressor(**best_params, random_state=42, n_jobs=-1)\n",
    "        model.fit(X_scaled, y_clean)\n",
    "        \n",
    "        # Store model and evaluate\n",
    "        self.models[target] = model\n",
    "        \n",
    "        # Cross-validation metrics\n",
    "        tscv = TimeSeriesSplit(n_splits=5)\n",
    "        cv_scores = cross_val_score(model, X_scaled, y_clean, cv=tscv, \n",
    "                                   scoring='neg_mean_squared_error')\n",
    "        \n",
    "        metrics = {\n",
    "            'cv_rmse': np.sqrt(-cv_scores.mean()),\n",
    "            'cv_std': cv_scores.std(),\n",
    "            'n_samples': len(y_clean),\n",
    "            'best_params': best_params\n",
    "        }\n",
    "        \n",
    "        self.training_metrics[target] = metrics\n",
    "        \n",
    "        # Feature importance\n",
    "        importance_dict = dict(zip(X_scaled.columns, model.feature_importances_))\n",
    "        self.feature_importance[target] = dict(\n",
    "            sorted(importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ {target} model trained - RMSE: {metrics['cv_rmse']:.3f}\")\n",
    "        return model\n",
    "    \n",
    "    def train_all_targets(self, features_df, targets_df, optimize=True):\n",
    "        \"\"\"Train models for all targets.\"\"\"\n",
    "        print(f\"üéØ Training {self.position} models...\")\n",
    "        \n",
    "        X, y = self.prepare_data(features_df, targets_df)\n",
    "        if X is None:\n",
    "            print(f\"‚ùå No data available for {self.position}\")\n",
    "            return\n",
    "        \n",
    "        print(f\"üìä Training data: {len(X)} samples, {len(X.columns)} features\")\n",
    "        \n",
    "        for target in self.target_columns:\n",
    "            if target in y.columns:\n",
    "                self.train_model(X, y, target, optimize)\n",
    "        \n",
    "        self.is_trained = True\n",
    "        print(f\"üéâ {self.position} training completed!\")\n",
    "    \n",
    "    def predict(self, features_df):\n",
    "        \"\"\"Make predictions.\"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"Model not trained\")\n",
    "        \n",
    "        position_mask = features_df['position'] == self.position\n",
    "        position_features = features_df[position_mask].copy()\n",
    "        \n",
    "        if position_features.empty:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        available_features = [col for col in self.feature_columns if col in position_features.columns]\n",
    "        X = position_features[available_features].fillna(0)\n",
    "        \n",
    "        predictions = {'player_id': position_features['player_id'].values}\n",
    "        \n",
    "        for target, model in self.models.items():\n",
    "            X_scaled = self.scalers[target].transform(X)\n",
    "            pred = model.predict(X_scaled)\n",
    "            predictions[f'pred_{target}'] = pred\n",
    "        \n",
    "        return pd.DataFrame(predictions)\n",
    "\n",
    "print(\"‚úÖ Base model class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell_4_position_configs"
   },
   "outputs": [],
   "source": [
    "# Cell 4: Position-Specific Model Configurations\n",
    "\n",
    "# Define position-specific configurations\n",
    "POSITION_CONFIGS = {\n",
    "    'Goalkeeper': {\n",
    "        'target_columns': ['total_points', 'clean_sheets', 'saves', 'bonus', 'minutes'],\n",
    "        'feature_columns': [\n",
    "            'form_points_5gw', 'form_points_3gw', 'form_minutes_5gw',\n",
    "            'clean_sheet_probability', 'average_saves_per_game', 'penalty_save_rate',\n",
    "            'fixture_difficulty', 'is_home', 'expected_goals_against',\n",
    "            'price_rank_in_team', 'is_key_player', 'price', 'ownership_percentage'\n",
    "        ],\n",
    "        'point_weights': {'clean_sheets': 4, 'saves': 1/3, 'bonus': 1, 'minutes': 2/90}\n",
    "    },\n",
    "    \n",
    "    'Defender': {\n",
    "        'target_columns': ['total_points', 'clean_sheets', 'goals_scored', 'assists', 'bonus', 'minutes'],\n",
    "        'feature_columns': [\n",
    "            'form_points_5gw', 'form_goals_5gw', 'form_assists_5gw',\n",
    "            'clean_sheet_probability', 'goal_scoring_rate', 'attacking_threat',\n",
    "            'fixture_difficulty', 'is_home', 'expected_goals_against',\n",
    "            'set_piece_likelihood', 'price_rank_in_team', 'price', 'ownership_percentage'\n",
    "        ],\n",
    "        'point_weights': {'clean_sheets': 4, 'goals_scored': 6, 'assists': 3, 'bonus': 1}\n",
    "    },\n",
    "    \n",
    "    'Midfielder': {\n",
    "        'target_columns': ['total_points', 'goals_scored', 'assists', 'bonus', 'minutes', 'clean_sheets'],\n",
    "        'feature_columns': [\n",
    "            'form_points_5gw', 'form_goals_5gw', 'form_assists_5gw',\n",
    "            'goal_scoring_rate', 'assist_rate', 'creativity_index',\n",
    "            'fixture_difficulty', 'is_home', 'expected_goals_for',\n",
    "            'set_piece_likelihood', 'penalty_likelihood', 'price', 'ownership_percentage'\n",
    "        ],\n",
    "        'point_weights': {'goals_scored': 5, 'assists': 3, 'clean_sheets': 1, 'bonus': 1}\n",
    "    },\n",
    "    \n",
    "    'Forward': {\n",
    "        'target_columns': ['total_points', 'goals_scored', 'assists', 'bonus', 'minutes'],\n",
    "        'feature_columns': [\n",
    "            'form_points_5gw', 'form_goals_5gw', 'form_assists_5gw',\n",
    "            'goal_scoring_rate', 'attacking_threat', 'ict_index',\n",
    "            'fixture_difficulty', 'is_home', 'expected_goals_for',\n",
    "            'penalty_likelihood', 'price_rank_in_team', 'price', 'ownership_percentage'\n",
    "        ],\n",
    "        'point_weights': {'goals_scored': 4, 'assists': 3, 'bonus': 1}\n",
    "    }\n",
    "}\n",
    "\n",
    "def calculate_fpl_points(predictions, position):\n",
    "    \"\"\"Calculate FPL points from component predictions.\"\"\"\n",
    "    config = POSITION_CONFIGS[position]\n",
    "    weights = config['point_weights']\n",
    "    \n",
    "    total_points = 0\n",
    "    \n",
    "    # Appearance points (2 for 60+ minutes)\n",
    "    if 'pred_minutes' in predictions.columns:\n",
    "        appearance_prob = np.clip(predictions['pred_minutes'] / 60, 0, 1)\n",
    "        total_points += appearance_prob * 2\n",
    "    \n",
    "    # Position-specific points\n",
    "    for component, weight in weights.items():\n",
    "        pred_col = f'pred_{component}'\n",
    "        if pred_col in predictions.columns:\n",
    "            total_points += predictions[pred_col] * weight\n",
    "    \n",
    "    return np.maximum(total_points, 0)  # Ensure non-negative\n",
    "\n",
    "print(\"‚úÖ Position configurations defined!\")\n",
    "print(f\"üìã Positions: {list(POSITION_CONFIGS.keys())}\")\n",
    "\n",
    "# Display feature counts by position\n",
    "for position, config in POSITION_CONFIGS.items():\n",
    "    print(f\"  {position}: {len(config['feature_columns'])} features, {len(config['target_columns'])} targets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell_5_train_models"
   },
   "outputs": [],
   "source": [
    "# Cell 5: Train All Position-Specific Models\n",
    "\n",
    "print(\"üéØ Starting position-specific model training...\")\n",
    "print(\"‚è±Ô∏è Estimated time: 20-30 minutes with optimization\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check if data is available\n",
    "if features_df.empty or targets_df.empty:\n",
    "    print(\"‚ùå Required data not available. Please run previous notebooks first.\")\nelse:\n",
    "    # Initialize trained models dictionary\n",
    "    trained_models = {}\n",
    "    \n",
    "    # Train models for each position\n",
    "    for position, config in POSITION_CONFIGS.items():\n",
    "        print(f\"\\nüéØ Training {position} Model\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Check if position has data\n",
    "        position_data = features_df[features_df['position'] == position]\n",
    "        if position_data.empty:\n",
    "            print(f\"‚ö†Ô∏è No {position} data found, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # Initialize position model\n",
    "        model = FPLPositionModel(\n",
    "            position=position,\n",
    "            target_columns=config['target_columns'],\n",
    "            feature_columns=config['feature_columns']\n",
    "        )\n",
    "        \n",
    "        # Train the model (with optimization for demonstration)\n",
    "        try:\n",
    "            model.train_all_targets(features_df, targets_df, optimize=True)\n",
    "            trained_models[position] = model\n",
    "            \n",
    "            # Display training results\n",
    "            print(f\"\\nüìä {position} Training Results:\")\n",
    "            for target, metrics in model.training_metrics.items():\n",
    "                print(f\"  {target}: RMSE={metrics['cv_rmse']:.3f}, Samples={metrics['n_samples']}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error training {position} model: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"üéâ Model training completed!\")\n",
    "    print(f\"‚úÖ Successfully trained: {list(trained_models.keys())}\")\n",
    "    \n",
    "    # Save training summary\n",
    "    training_summary = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'trained_positions': list(trained_models.keys()),\n",
    "        'total_models': sum(len(model.models) for model in trained_models.values()),\n",
    "        'performance_summary': {}\n",
    "    }\n",
    "    \n",
    "    for position, model in trained_models.items():\n",
    "        training_summary['performance_summary'][position] = {\n",
    "            target: metrics['cv_rmse'] for target, metrics in model.training_metrics.items()\n",
    "        }\n",
    "    \n",
    "    # Display performance summary\n",
    "    print(\"\\nüìà Performance Summary:\")\n",
    "    for position, performance in training_summary['performance_summary'].items():\n",
    "        avg_rmse = np.mean(list(performance.values()))\n",
    "        print(f\"  {position}: Avg RMSE = {avg_rmse:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell_6_model_evaluation"
   },
   "outputs": [],
   "source": [
    "# Cell 6: Model Evaluation and Feature Importance Analysis\n",
    "\n",
    "print(\"üìä Analyzing model performance and feature importance...\")\n",
    "\n",
    "if not trained_models:\n",
    "    print(\"‚ùå No trained models available for evaluation\")\nelse:\n",
    "    # Create comprehensive evaluation plots\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=['Model Performance by Position', 'Feature Importance - Goalkeeper', \n",
    "                       'Feature Importance - Midfielder', 'Feature Importance - Forward'],\n",
    "        specs=[[{\"type\": \"bar\"}, {\"type\": \"bar\"}],\n",
    "               [{\"type\": \"bar\"}, {\"type\": \"bar\"}]]\n",
    "    )\n",
    "    \n",
    "    # Performance comparison\n",
    "    positions = []\n",
    "    avg_rmse = []\n",
    "    \n",
    "    for position, model in trained_models.items():\n",
    "        if model.training_metrics:\n",
    "            rmse_values = [metrics['cv_rmse'] for metrics in model.training_metrics.values()]\n",
    "            positions.append(position)\n",
    "            avg_rmse.append(np.mean(rmse_values))\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(x=positions, y=avg_rmse, name=\"Average RMSE\",\n",
    "               text=[f\"{val:.3f}\" for val in avg_rmse], textposition=\"outside\"),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Feature importance for key positions\n",
    "    plot_positions = ['Goalkeeper', 'Midfielder', 'Forward']\n",
    "    plot_coords = [(1, 2), (2, 1), (2, 2)]\n",
    "    \n",
    "    for i, position in enumerate(plot_positions):\n",
    "        if position in trained_models:\n",
    "            model = trained_models[position]\n",
    "            \n",
    "            # Get feature importance for primary target (total_points)\n",
    "            if 'total_points' in model.feature_importance:\n",
    "                importance = model.feature_importance['total_points']\n",
    "                top_features = list(importance.keys())[:8]  # Top 8 features\n",
    "                importance_values = [importance[feat] for feat in top_features]\n",
    "                \n",
    "                row, col = plot_coords[i]\n",
    "                fig.add_trace(\n",
    "                    go.Bar(x=importance_values, y=top_features, orientation='h',\n",
    "                           name=f\"{position} Features\", showlegend=False),\n",
    "                    row=row, col=col\n",
    "                )\n",
    "    \n",
    "    fig.update_layout(height=800, title_text=\"FPL Model Evaluation Dashboard\")\n",
    "    fig.show()\n",
    "    \n",
    "    # Detailed performance metrics table\n",
    "    print(\"\\nüìã Detailed Performance Metrics:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    performance_data = []\n",
    "    for position, model in trained_models.items():\n",
    "        for target, metrics in model.training_metrics.items():\n",
    "            performance_data.append({\n",
    "                'Position': position,\n",
    "                'Target': target,\n",
    "                'CV_RMSE': f\"{metrics['cv_rmse']:.3f}\",\n",
    "                'Samples': metrics['n_samples'],\n",
    "                'Features': len(model.feature_columns)\n",
    "            })\n",
    "    \n",
    "    performance_df = pd.DataFrame(performance_data)\n",
    "    print(performance_df.to_string(index=False))\n",
    "    \n",
    "    # Feature importance summary\n",
    "    print(\"\\nüîù Top Features by Position:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for position, model in trained_models.items():\n",
    "        if 'total_points' in model.feature_importance:\n",
    "            top_features = list(model.feature_importance['total_points'].keys())[:5]\n",
    "            print(f\"\\n{position}:\")\n",
    "            for i, feature in enumerate(top_features, 1):\n",
    "                importance_score = model.feature_importance['total_points'][feature]\n",
    "                print(f\"  {i}. {feature}: {importance_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell_7_save_models"
   },
   "outputs": [],
   "source": [
    "# Cell 7: Save Trained Models and Create Ensemble\n",
    "\n",
    "print(\"üíæ Saving trained models to Google Drive...\")\n",
    "\n",
    "if not trained_models:\n",
    "    print(\"‚ùå No trained models to save\")\nelse:\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Save individual position models\n",
    "    saved_models = {}\n",
    "    for position, model in trained_models.items():\n",
    "        model_file = f\"models/trained_models/{position.lower()}/{timestamp}_{position.lower()}_model.joblib\"\n",
    "        \n",
    "        # Prepare model data for saving\n",
    "        model_data = {\n",
    "            'position': position,\n",
    "            'models': model.models,\n",
    "            'scalers': model.scalers,\n",
    "            'feature_importance': model.feature_importance,\n",
    "            'training_metrics': model.training_metrics,\n",
    "            'target_columns': model.target_columns,\n",
    "            'feature_columns': model.feature_columns,\n",
    "            'timestamp': timestamp\n",
    "        }\n",
    "        \n",
    "        joblib.dump(model_data, model_file)\n",
    "        saved_models[position] = model_file\n",
    "        print(f\"‚úÖ {position} model saved: {model_file}\")\n",
    "    \n",
    "    # Create ensemble predictor class\n",
    "    class FPLEnsemblePredictor:\n",
    "        \"\"\"Ensemble predictor combining all position models.\"\"\"\n",
    "        \n",
    "        def __init__(self, model_files):\n",
    "            self.position_models = {}\n",
    "            self.load_models(model_files)\n",
    "        \n",
    "        def load_models(self, model_files):\n",
    "            \"\"\"Load all position models.\"\"\"\n",
    "            for position, file_path in model_files.items():\n",
    "                model_data = joblib.load(file_path)\n",
    "                self.position_models[position] = model_data\n",
    "                print(f\"üìÇ Loaded {position} model\")\n",
    "        \n",
    "        def predict_gameweek(self, features_df, gameweek=None):\n",
    "            \"\"\"Predict points for all players for a gameweek.\"\"\"\n",
    "            all_predictions = []\n",
    "            \n",
    "            for position, model_data in self.position_models.items():\n",
    "                # Filter features for this position\n",
    "                position_features = features_df[features_df['position'] == position].copy()\n",
    "                \n",
    "                if position_features.empty:\n",
    "                    continue\n",
    "                \n",
    "                # Prepare features\n",
    "                available_features = [col for col in model_data['feature_columns'] \n",
    "                                    if col in position_features.columns]\n",
    "                X = position_features[available_features].fillna(0)\n",
    "                \n",
    "                # Make predictions for each target\n",
    "                predictions = {\n",
    "                    'player_id': position_features['player_id'].values,\n",
    "                    'position': position,\n",
    "                    'gameweek': gameweek or position_features.get('gameweek', 0).iloc[0]\n",
    "                }\n",
    "                \n",
    "                for target, model in model_data['models'].items():\n",
    "                    scaler = model_data['scalers'][target]\n",
    "                    X_scaled = scaler.transform(X)\n",
    "                    pred = model.predict(X_scaled)\n",
    "                    predictions[f'pred_{target}'] = pred\n",
    "                \n",
    "                # Calculate total FPL points\n",
    "                pred_df = pd.DataFrame(predictions)\n",
    "                pred_df['predicted_points'] = calculate_fpl_points(pred_df, position)\n",
    "                \n",
    "                all_predictions.append(pred_df)\n",
    "            \n",
    "            return pd.concat(all_predictions, ignore_index=True) if all_predictions else pd.DataFrame()\n",
    "        \n",
    "        def get_top_players(self, features_df, position=None, top_n=10):\n",
    "            \"\"\"Get top predicted players.\"\"\"\n",
    "            predictions = self.predict_gameweek(features_df)\n",
    "            \n",
    "            if position:\n",
    "                predictions = predictions[predictions['position'] == position]\n",
    "            \n",
    "            return predictions.nlargest(top_n, 'predicted_points')\n",
    "    \n",
    "    # Create and save ensemble predictor\n",
    "    ensemble = FPLEnsemblePredictor(saved_models)\n",
    "    ensemble_file = f\"models/ensemble/{timestamp}_fpl_ensemble.joblib\"\n",
    "    joblib.dump(saved_models, ensemble_file)  # Save model file paths\n",
    "    \n",
    "    print(f\"\\nüéØ Ensemble predictor created and saved: {ensemble_file}\")\n",
    "    \n",
    "    # Test ensemble on sample data\n",
    "    print(\"\\nüß™ Testing ensemble predictor...\")\n",
    "    sample_features = features_df.head(20)  # Test on first 20 players\n",
    "    sample_predictions = ensemble.predict_gameweek(sample_features, gameweek=1)\n",
    "    \n",
    "    if not sample_predictions.empty:\n",
    "        print(\"‚úÖ Ensemble test successful!\")\n",
    "        print(f\"üìä Sample predictions shape: {sample_predictions.shape}\")\n",
    "        \n",
    "        # Show top predictions by position\n",
    "        print(\"\\nüèÜ Top Predicted Players by Position:\")\n",
    "        for position in ['Goalkeeper', 'Defender', 'Midfielder', 'Forward']:\n",
    "            pos_preds = sample_predictions[sample_predictions['position'] == position]\n",
    "            if not pos_preds.empty:\n",
    "                top_player = pos_preds.nlargest(1, 'predicted_points')\n",
    "                if not top_player.empty:\n",
    "                    player_id = top_player['player_id'].iloc[0]\n",
    "                    points = top_player['predicted_points'].iloc[0]\n",
    "                    print(f\"  {position}: Player {player_id} - {points:.2f} points\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Ensemble test returned no predictions\")\n",
    "    \n",
    "    # Create final model summary\n",
    "    model_summary = {\n",
    "        'timestamp': timestamp,\n",
    "        'positions_trained': list(saved_models.keys()),\n",
    "        'total_individual_models': sum(len(model.models) for model in trained_models.values()),\n",
    "        'ensemble_file': ensemble_file,\n",
    "        'model_files': saved_models,\n",
    "        'next_steps': [\n",
    "            \"Create prediction dashboard\",\n",
    "            \"Implement backtesting\",\n",
    "            \"Add real-time data updates\",\n",
    "            \"Deploy to production\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Save summary\n",
    "    summary_file = f\"models/{timestamp}_model_summary.json\"\n",
    "    with open(summary_file, 'w') as f:\n",
    "        json.dump(model_summary, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nüìã Model summary saved: {summary_file}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üéâ MODEL TRAINING COMPLETE!\")\n",
    "    print(\"üöÄ Ready for prediction dashboard and backtesting!\")\n",
    "    print(f\"üìÅ All models saved in: {os.getcwd()}/models/\")\n",
    "    print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 }
}